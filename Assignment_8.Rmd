---
title: "Assignment 8"
author: "Mia Thuge Kuntz"
date: "`r Sys.Date()`"
output: html_document
---

# Assigment 8 - Task 1
I have chosen the first task of the assignment, where I will reproduce the code from this weeks lecture to be able to consider the sentiment in the Game of Thrones book.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

get_sentiments(lexicon = "nrc")
get_sentiments(lexicon = "afinn")

```

## Extracting the text in the book
To extract the text from the GoT book, I assign it to its own object, which I then assign to another object, where I filter away words that aren't "selectable."
```{r get-document}
got_path <- here("8","got.pdf")
got_text <- pdf_text(got_path)
```

### Checking the text
To check whether the text I extracted actually matches the text in the book, I can extract the text from a single page, which I then compare to my .pdf of the book.
```{r single-page}
got_p10 <- got_text[10]
got_p10
```

Luckily it matches, although the R extract of the page differs in relation to the separation of lines!

## Wrangling the text object
I am now working with a vector of strings, where each one of the pages in the book is its own vector. To work with the text for the rest of the assignment, I convert it into a data frame using the `data.frame` function. I then use the `str_split` function to make individual lines from the pages, where the split happens each time a backslash-n occurs. To keep track of everything, I add a line number in addition to the page number the line appears on. The `unnest`function unnests it into regular columns, and lastly I mutate the functions together. 
```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```

## Tidying up the individual words
Using the `unnest_tokens` function I split each column into a token, where the token I use is *words*. 
```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```

### Counting the words and cleaning up stop-words
At first glance at the output of the function above, many of the appearing words can be describes as *stop-words*. To see which words occurs the most, I use the `count(word)` funtion and arrange it in descending order.
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```

Some of the words appearing the most are indeed stop-words. To further look into the sentiment of the book, I choose to remove these using the `anti_join` function.
```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```

I now check to make sure the function did it's job and use the `count` function once more.
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
```

To further clean up the text, I choose to remove all non-text numbers, such as page numbers and those appearing in the actual plot of the book. I here use a `filter` function to make this happen.
```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

Now the most used words when counted are those actually relevant to my analysis!

#### Word cloud
To visualize my word cleaning I now produce a word cloud to show the most used word in the book after I have removed stop-words and numbers. I first check to see how many unique words appear in the text, so that I can filter the cloud to only show the top 50 of appearing words. 
```{r wordcloud-prep}
length(unique(got_no_numeric$word))

got_top50 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(50)
```

With 11209 unique words, it was probably a good idea to only include the top 50 most used to have the cloud appear easier to read. To actually produce the word cloud, I use the `geom_text_wordcloud` ggplot function, and thereafter create the theme and other aesthetics I would like to have it appear in.

```{r wordcloud}
got_cloud <- ggplot(data = got_top50, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()

got_cloud
```

The cloud gives me a clear view of which words appear the most, which are "lord" "ser" "jon" and "ned". But to actually understand the sentiment behind the most used words, I need to actually use some of the recommended tools. 

## The sentiment analysis
There are different sentiment lexicons to use when analyzing the words of a text, depending on the purpose of the analysis. For this assignment, I will make use of the two provided lexicons from the tutorial, which are:

  -  AFINN from Finn Ã…rup Nielsen,
  -  nrc from Saif Mohammad and Peter Turney

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

For all lexicons, I will first use the function `get_sentiments` to be able to work with the chosen lexicon.

### The AFINN lexicon
The AFINN lexicon ranks a word sentiment from -5 (very negative) to +5 (very positive). I first bind the words of the GoT text to this lexicon by using the `inner_join` function, which will be done throughout the rest of the task every time I start working on a new lexicon.
```{r afinn}
get_sentiments(lexicon = "afinn")
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

I am now able to count and plot the words depending on their sentiment ranking.
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

This shows, that the words ranked -2 appear the most in the book! To see which words these are, I filter for them, then check, and then I can count and plot them to visualize them. 
```{r afinn-2}
got_afinn_minus_2 <- got_afinn %>% 
  filter(value == -2)

unique(got_afinn_minus_2$word)

got_afinn_minus_2_n <- got_afinn_minus_2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))

ggplot(data = got_afinn_minus_2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()
```

The plot is somewhat unreadable, but the created object shows that the most used word with the -2 ranking is "fire". In this book, the word fire could be associated with something positive, since it brings warmth, but for the most part it matches with the analysis as it being a somewhat negative word. 

As a final way of analyzing this lexicons way of rating the words of the book, I can look at how it ranks the book as a whole by looking at both the mean and the median of the words.
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
got_summary
```

According to the AFINN lexicon, the book tends slighty towards more negative words.

### The nrc lexicon
The nrc lexicon sorts the words of a text into 8 different emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, trust, along with positive and negative. 
```{r bind-nrc}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
get_sentiments(lexicon = "nrc")
```

For this lexicon, I would like to check which words are being excluded from the analysis to see, whether the lexicon filters them according to what I had in mind.
```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

An interesting find! Here, the bing lexicon filters away some of those words, which are most used in the text. This is good to keep in mind for the remainder of the analysis.

I will now count the number of words appearing in the different sentiment categories and plot them.
```{r count-nrc}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)
ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```

It sorts most of the words into the negative and positive categories. 

To get a clearer look on which words appear most in which categories, I count them by sentiment and word, and then facet and plot it for visualization. 
```{r nrc_n5}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("8","got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)
```

This shows, that the word "lord" appears as the most used word in 4 of the categories: disgust, negative, positive and trust. So very different opinions on what sentiment this word can be categorized as.

As expected for the first book in the great series of A Song of Ice and Fire, both positive and negative words dominate the sentiment, with words associated with trust also showing up quite a lot. 
